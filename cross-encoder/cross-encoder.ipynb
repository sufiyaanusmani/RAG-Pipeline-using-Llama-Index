{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embed_model = OpenAIEmbedding(model_name=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4o-mini\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorstore import get_vectorstore\n",
    "\n",
    "vectorstore = get_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from index import get_index\n",
    "\n",
    "index = get_index(embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store_docs(docs, filename):\n",
    "    docs_to_store = []\n",
    "\n",
    "    for node in docs:\n",
    "        docs_to_store.append({\n",
    "            \"text\": node.text,\n",
    "            \"metadata\": node.metadata,\n",
    "            \"score\": node.score\n",
    "        })\n",
    "\n",
    "    with open(f\"./output/{filename}\", \"w\") as f:\n",
    "        json.dump(docs_to_store, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt = \"\"\"You are an AI language model assistant specializing in query expansion. Your task is to generate {num_queries} diverse versions of the given user question. These variations will be used to retrieve relevant documents from a vector database, helping to overcome limitations of distance-based similarity search.\n",
    "\n",
    "Original question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Create {num_queries} unique variations of the original question.\n",
    "2. Ensure each variation maintains the core intent of the original question.\n",
    "3. Use different phrasings, synonyms, or perspectives for each variation.\n",
    "4. Consider potential context or implications not explicitly stated in the original question.\n",
    "5. Avoid introducing new topics or drastically changing the meaning of the question.\n",
    "\n",
    "Please provide your {num_queries} question variations, each on a new line:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 50\n",
    "num_queries = 5\n",
    "question = \"Who is Ali?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = index.as_retriever(similarity_top_k=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever],\n",
    "    similarity_top_k=top_n,\n",
    "    num_queries=num_queries,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    query_gen_prompt=query_gen_prompt,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What is the identity of Ali?\n",
      "2. Can you provide information about Ali?\n",
      "3. Who goes by the name Ali?\n",
      "4. What can you tell me about Ali?\n"
     ]
    }
   ],
   "source": [
    "# it will genereate n - 1 queries since the original query is also included\n",
    "nodes_with_scores = retriever.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_docs(nodes_with_scores, \"baseline_docs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "sentence_transformer_rerank = SentenceTransformerRerank(\n",
    "    model='cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    top_n=top_n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder_retriever = index.as_retriever(similarity_top_k=top_n, postprocessor=sentence_transformer_rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = cross_encoder_retriever.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = index.as_retriever(similarity_top_k=top_n, postprocessors=[sentence_transformer_rerank])\n",
    "# b = a.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_docs(docs, \"cross_encoder_docs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Encoder with Query Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "cross_encoder_retriever_with_query_fusion = QueryFusionRetriever(\n",
    "    [cross_encoder_retriever],\n",
    "    similarity_top_k=top_n,\n",
    "    num_queries=num_queries,  # set this to 1 to disable query generation\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    query_gen_prompt=query_gen_prompt,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What is the identity of Ali?\n",
      "2. Can you provide information about Ali?\n",
      "3. Who goes by the name Ali?\n",
      "4. What can you tell me about Ali?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2 = cross_encoder_retriever_with_query_fusion.retrieve(question)\n",
    "len(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_docs(docs2, \"cross_encoder_retriever_with_query_fusion.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
