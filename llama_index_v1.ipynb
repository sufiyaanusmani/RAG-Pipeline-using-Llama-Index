{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Installing Libraries\n",
    "\n",
    "Reference: [Llama Index Installation and Setup](https://docs.llamaindex.ai/en/stable/getting_started/installation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv llama-index chromadb llama-index-vector-stores-chroma llama-index-retrievers-bm25 EbookLib html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate, get_response_synthesizer\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.prompts import SelectorPromptTemplate\n",
    "\n",
    "from ebooklib import epub\n",
    "import uuid\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Importing Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Setting up Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Setting up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4o-mini\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up IndexedVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name \"IndexedVectorStore\" emphasizes that the class handles both the vector store and the index\n",
    "\n",
    "class IndexedVectorStore:\n",
    "    def __init__(self):\n",
    "        self.db = chromadb.PersistentClient(path=\"./db\")\n",
    "        self.chroma_collection = self.db.get_or_create_collection(\"transcription_project\")\n",
    "        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)\n",
    "        self.index = VectorStoreIndex.from_vector_store(\n",
    "            self.vector_store,\n",
    "            embed_model=embed_model,\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: list) -> None:\n",
    "        # Add the documents to the LlamaIndex and persist them\n",
    "        for document in documents:\n",
    "            self.index.insert(document)\n",
    "        self.index.storage_context.persist(persist_dir=\"./db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = IndexedVectorStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading Data from Directory using `SimpleDirectoryReader`\n",
    "\n",
    "Reference: [Loaders](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Metadata Reference: [SimpleDirectoryReader](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a function that will read each file and extract metadata that gets attached to the resulting Document object for each file by passing the function as `file_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epub_metadata(book_path: str) -> dict:\n",
    "    book_path = Path(book_path)\n",
    "    if not book_path.exists():\n",
    "        raise FileNotFoundError(f\"EPUB file not found at path: {book_path}\")\n",
    "    book = epub.read_epub(str(book_path))\n",
    "\n",
    "    return {\n",
    "        \"id\": f\"epub-{uuid.uuid4().hex}\",\n",
    "        \"title\": book.get_metadata(\"DC\", \"title\")[0][0].rstrip(\".epub\") if book.get_metadata(\"DC\", \"title\") else \"N/A\",\n",
    "        \"author\": book.get_metadata(\"DC\", \"creator\")[0][0] if book.get_metadata(\"DC\", \"creator\") else \"\",\n",
    "        \"language\": book.get_metadata(\"DC\", \"language\")[0][0] if book.get_metadata(\"DC\", \"language\") else \"\",\n",
    "        \"description\": book.get_metadata(\"DC\", \"description\")[0][0] if book.get_metadata(\"DC\", \"description\") else \"\",\n",
    "        \"type\": \"epub\",\n",
    "        \"embeddings\": \"openaiembeddings\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Capelin\\llama-index-test\\.venv\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "e:\\Capelin\\llama-index-test\\.venv\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_dir=\"./data\", file_metadata=extract_epub_metadata).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'epub-1b5ac705d1b54cf580ddb53e7356f398', 'title': \"Theological Instructions (Amuzish-e Aqa'id)\", 'author': 'Muhammad Taqi Misbah Yazdi', 'language': 'en', 'description': '', 'type': 'epub', 'embeddings': 'openaiembeddings'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a new book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata of first element: {'id': 'epub-6d7d7179a80e414c9c18e6715cae457f', 'title': 'Give and Tak', 'author': 'Unknown', 'language': 'en', 'description': '', 'type': 'epub'}\n"
     ]
    }
   ],
   "source": [
    "new_book = SimpleDirectoryReader(input_files=[\"./data/give_and_take.epub\"], file_metadata=extract_epub_metadata).load_data()\n",
    "print(f\"Metadata of first element: {new_book[0].metadata}\")\n",
    "\n",
    "# This way, we can load a new book and can use the same VectorStore object to add the new book to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is loaded, you then need to process and transform your data before putting it into a storage system. These transformations include chunking, extracting metadata, and embedding each chunk. This is necessary to make sure that the data can be retrieved, and used optimally by the LLM.\n",
    "\n",
    "An `IngestionPipeline` uses a concept of Transformations that are applied to input data. These Transformations are applied to your input data\n",
    "\n",
    "Reference: [IngestionPipeline](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 512, Overlap Percentage: 0.25, Chunk Overlap: 128\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 512\n",
    "overlap_percentage = 0.25\n",
    "chunk_overlap = int(chunk_size * overlap_percentage)\n",
    "\n",
    "print(f\"Chunk Size: {chunk_size}, Overlap Percentage: {overlap_percentage}, Chunk Overlap: {chunk_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap),\n",
    "        embed_model # OpenAIEmbedding\n",
    "    ],\n",
    "    vector_store=vectorstore.vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents: 513\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load and add a new book to vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_book(book_path: str):\n",
    "    print(f\"Loading book from path: {book_path}\")\n",
    "    new_book = SimpleDirectoryReader(input_files=[book_path], file_metadata=extract_epub_metadata).load_data()\n",
    "    print(f\"Loaded book with metadata: {new_book[0].metadata}\")\n",
    "    new_book = pipeline.run(documents=new_book)\n",
    "    print(\"Book added successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading book from path: ./data/give_and_take.epub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Capelin\\llama-index-test\\.venv\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "e:\\Capelin\\llama-index-test\\.venv\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded book with metadata: {'id': 'epub-b09eb4f758f54495a29b6b23763dffb3', 'title': 'Give and Tak', 'author': 'Unknown', 'language': 'en', 'description': '', 'type': 'epub'}\n",
      "Book added successfully!\n"
     ]
    }
   ],
   "source": [
    "add_book(\"./data/give_and_take.epub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Query Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"You are an AI language model assistant specializing in query expansion. Your task is to generate {num_queries} diverse versions of the given user question. These variations will be used to retrieve relevant documents from a vector database, helping to overcome limitations of distance-based similarity search.\n",
    "\n",
    "Original question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. Create {num_queries} unique variations of the original question.\n",
    "2. Ensure each variation maintains the core intent of the original question.\n",
    "3. Use different phrasings, synonyms, or perspectives for each variation.\n",
    "4. Consider potential context or implications not explicitly stated in the original question.\n",
    "5. Avoid introducing new topics or drastically changing the meaning of the question.\n",
    "\n",
    "Please provide your {num_queries} question variations, each on a new line:\n",
    "\"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(question_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_variations(question: str, num_queries: int):\n",
    "    print(f\"Generating query variations for: {question}\")\n",
    "\n",
    "    fmt_prompt = question_prompt.format(num_queries=num_queries, query=question)\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "\n",
    "    print(\"Generated query variations:\")\n",
    "    for query in queries:\n",
    "        print(f\"  {query}\")\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating query variations for: Why is there only one God?\n",
      "Generated query variations:\n",
      "  1. What is the rationale behind the belief in a singular God?\n",
      "  2. How do monotheistic religions justify the existence of only one deity?\n",
      "  3. What factors contribute to the concept of a solitary divine being?\n",
      "  4. Is there a specific reason for the monotheistic view of a singular God?\n",
      "  5. What leads to the idea that there can be only one supreme being in various faiths?\n"
     ]
    }
   ],
   "source": [
    "question = \"Why is there only one God?\"\n",
    "num_queries = 5\n",
    "query_variations = generate_query_variations(question, num_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Performing Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "\n",
    "vector_retriever = vectorstore.index.as_retriever(similarity_top_k=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever],\n",
    "    similarity_top_k=top_n,\n",
    "    num_queries=num_queries,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What are the reasons for monotheism in various religions?\n",
      "2. How do different cultures and belief systems explain the concept of a single deity?\n",
      "3. Are there any philosophical arguments for the existence of a singular supreme being?\n",
      "4. How does the concept of one God differ across different religious traditions?\n"
     ]
    }
   ],
   "source": [
    "nodes_with_scores = retriever.retrieve(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- BOOK INFO -------\n",
      "Book Title: Theological Instructions (Amuzish-e Aqa'id)\n",
      "Book ID   : epub-1b5ac705d1b54cf580ddb53e7356f398\n",
      "Author    : Muhammad Taqi Misbah Yazdi\n",
      "\n",
      "------- TEXT -------\n",
      "By explaining the\n",
      "contradiction in this belief, it is possible to nullify the argument of those\n",
      "who held such a view.\n",
      "\n",
      "In order to establish the oneness of God, the Supreme, many arguments have\n",
      "been demonstrated in the different books of theology and philosophy. Here we\n",
      "are going to demonstrate an argument, which encompasses the oneness of\n",
      "lordship and rejects the polytheistic beliefs.\n",
      "\n",
      "## Proofs For The Oneness Of God\n",
      "\n",
      "The assumption that the universe has two or more gods can solely be imagined\n",
      "through a few possibilities:\n",
      "\n",
      "Firstly, it can be considered that every phenomenon of the universe is created\n",
      "and is an effect of all the assumed gods. The second assumption could be that\n",
      "each particular group of phenomena is an effect (or created by a particular\n",
      "god) of one of the assumed gods. Finally, the third assumption is that all of\n",
      "the phenomena are created by one of these assumed gods and the other gods are\n",
      "recognized as the managers of the universe.\n",
      "\n",
      "However, it is impossible to assume that every phenomenon has several gods. If\n",
      "two or more gods created an existent, it would imply that each of the assumed\n",
      "gods would create an existent. This would result in many existents, whereas in\n",
      "reality there is only one.\n",
      "\n",
      "If it is to be assumed that a particular god creates each particular\n",
      "phenomenon, this will imply that each phenomenon exists because of its\n",
      "particular god. Furthermore, they must not require or depend upon any other\n",
      "existent unless the (dependency) need returns to their particular god. This\n",
      "type of requirement or need must be upon the existent, which is created by\n",
      "that very creator who has created that particular group.\n",
      "\n",
      "In other words, the assumption of having more than one god necessitates the\n",
      "order in the universe to be multifarious and deteriorating. In reality there\n",
      "is only one order and all phenomena are related and effectual upon each other\n",
      "and at the same time need each other.\n",
      "\n",
      "Furthermore, the present phenomenon is linked with the former phenomenon and\n",
      "every coexisting phenomenon creates grounds for future phenomena.\n"
     ]
    }
   ],
   "source": [
    "# Printing first document\n",
    "print(\"------- BOOK INFO -------\")\n",
    "print(f\"Book Title: {nodes_with_scores[0].metadata['title']}\")\n",
    "print(f\"Book ID   : {nodes_with_scores[0].metadata['id']}\")\n",
    "print(f\"Author    : {nodes_with_scores[0].metadata['author']}\")\n",
    "\n",
    "print(\"\\n------- TEXT -------\")\n",
    "print(nodes_with_scores[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_template = \"\"\"You are a knowledgeable AI assistant tasked with answering questions based on the provided context. Your goal is to provide a comprehensive, accurate, and well-structured response using Chain-of-Thought reasoning.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "Instructions:\n",
    "1. Carefully analyze the given context and question.\n",
    "2. Use Chain-of-Thought reasoning to break down your answer into clear steps:\n",
    "   a. First, identify the key components of the question, such as sub-problems that need to be explained before an answer can be derived\n",
    "   b. Then, for each component, explain your thought process as you analyze the relevant information from the context.\n",
    "   c. Show how you're connecting different pieces of information to form your conclusion.\n",
    "3. Provide a detailed answer using only the information from the context.\n",
    "4. If the context doesn't contain enough information to fully answer the question, state this clearly and explain why.\n",
    "5. Organize your response with appropriate headings and subheadings for clarity.\n",
    "6. Use bullet points or numbered lists where applicable to improve readability.\n",
    "7. If relevant, include brief examples or analogies to illustrate key points.\n",
    "8. After your detailed Chain-of-Thought reasoning, summarize your main points at the end of the response.\n",
    "9. At the end, list all the contexts used in your reasoning. After your response, add a \"References\" section where you list the full contexts that you used arrive at your answer. Provide as much detail as available from each context (e.g., book title, author, full text of the relevant contexts. For video sources, include the url to the video.\n",
    "\n",
    "For the references, use the format:\n",
    "\n",
    "# References:\n",
    "(for each context:)\n",
    "## Context Id: title\n",
    "Context excerpt (print as it is)\n",
    "\n",
    "Please format your entire response in markdown for optimal readability.\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = PromptTemplate(answer_template)\n",
    "answer_prompt_sel = SelectorPromptTemplate(answer_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Modes\n",
    "\n",
    "`REFINE`:\n",
    "\n",
    "Refine is an iterative way of generating a response. We first use the context in the first node, along with the query, to generate an initial answer. We then pass this answer, the query, and the context of the second node as input into a “refine prompt” to generate a refined answer. We refine through N-1 nodes, where N is the total number of nodes.\n",
    "\n",
    "\n",
    "`COMPACT`:\n",
    "\n",
    "Compact and refine mode first combine text chunks into larger consolidated chunks that more fully utilize the available context window, then refine answers across them. This mode is faster than refine since we make fewer calls to the LLM.\n",
    "\n",
    "\n",
    "`SIMPLE_SUMMARIZE`:\n",
    "\n",
    "Merge all text chunks into one, and make a LLM call. This will fail if the merged text chunk exceeds the context window size.\n",
    "\n",
    "\n",
    "`TREE_SUMMARIZE`:\n",
    "\n",
    "Build a tree index over the set of candidate nodes, with a summary prompt seeded with the query. The tree is built in a bottoms-up fashion, and in the end the root node is returned as the response\n",
    "\n",
    "\n",
    "`GENERATION`:\n",
    "\n",
    "Ignore context, just use LLM to generate a response.\n",
    "\n",
    "`NO_TEXT`:\n",
    "\n",
    "Return the retrieved context nodes, without synthesizing a final response.\n",
    "\n",
    "`CONTEXT_ONLY`:\n",
    "\n",
    "Returns a concatenated string of all text chunks.\n",
    "\n",
    "`ACCUMULATE`:\n",
    "\n",
    "Synthesize a response for each text chunk, and then return the concatenation.\n",
    "\n",
    "`COMPACT_ACCUMULATE`:\n",
    "\n",
    "Compact and accumulate mode first combine text chunks into larger consolidated chunks that more fully utilize the available context window, then accumulate answers for each of them and finally return the concatenation. This mode is faster than accumulate since we make fewer calls to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, documents, answer_prompt):\n",
    "    response_synthesizer = get_response_synthesizer(response_mode=\"compact\", llm=llm, text_qa_template=answer_prompt)\n",
    "\n",
    "    response = response_synthesizer.synthesize(\n",
    "        question, nodes=documents\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_md = generate_answer(question, nodes_with_scores, answer_prompt_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Answer:\n",
      "\n",
      "## Key Components:\n",
      "1. Understanding the concept of monotheism and the rejection of polytheism.\n",
      "2. Exploring the arguments against the existence of multiple gods.\n",
      "3. Analyzing the relationship between creatorship, lordship, and the oneness of God.\n",
      "4. Examining the illusion of several gods and how it can be nullified.\n",
      "\n",
      "## Chain-of-Thought Reasoning:\n",
      "\n",
      "### 1. Monotheism vs. Polytheism:\n",
      "- The context emphasizes the importance of establishing the oneness of God to reject polytheistic beliefs.\n",
      "- Polytheism arises from various factors like idol worship, human tendencies, and the influence of tyrants.\n",
      "- Monotheism asserts the belief in one God as the sole creator and lord of the universe.\n",
      "\n",
      "### 2. Arguments Against Multiple Gods:\n",
      "- The text presents arguments against the existence of multiple gods creating the universe.\n",
      "- It refutes the idea of each phenomenon having several gods, as it would lead to a multitude of existents, contrary to the observed unity in creation.\n",
      "- The complexity and interconnectedness of the universe suggest a single source of creation and administration.\n",
      "\n",
      "### 3. Creatorship, Lordship, and Oneness of God:\n",
      "- The text highlights the inseparability of creatorship and true lordship.\n",
      "- Belief in the creatorship of God and acceptance of lordship by others are deemed incompatible.\n",
      "- Recognizing multiple lords or administrators for creation contradicts the unity and order observed in the universe.\n",
      "\n",
      "### 4. Nullifying the Illusion of Several Gods:\n",
      "- The illusion of several gods stems from misconceptions about the causes of existence and administration of the universe.\n",
      "- By contemplating the coherence and order in creation, one can realize the impossibility of multiple gods creating and managing the universe.\n",
      "- Accepting Divine authority without negating the oneness of God further reinforces the rejection of polytheism.\n",
      "\n",
      "## Summary:\n",
      "- The concept of monotheism, as opposed to polytheism, is rooted in the belief in the oneness of God as the sole creator and lord of the universe.\n",
      "- Arguments against multiple gods creating and administering the universe highlight the unity, coherence, and order observed in creation.\n",
      "- The inseparability of creatorship and lordship underscores the incompatibility of accepting other lords alongside the belief in one God.\n",
      "- By nullifying the illusion of several gods through contemplation and understanding of Divine authority, the oneness of God is reaffirmed.\n",
      "\n",
      "# References:\n",
      "## Context Id: epub-1b5ac705d1b54cf580ddb53e7356f398\n",
      "By explaining the contradiction in this belief, it is possible to nullify the argument of those who held such a view.\n",
      "In order to establish the oneness of God, the Supreme, many arguments have been demonstrated in the different books of theology and philosophy. Here we are going to demonstrate an argument, which encompasses the oneness of lordship and rejects the polytheistic beliefs.\n",
      "\n",
      "## Context Id: epub-1b5ac705d1b54cf580ddb53e7356f398\n",
      "Some related goodness to the god of good and evil to the god of evil. This resulted in the belief of there being two sources for the world.\n",
      "From another angle by focusing upon the effect of the light (_Nur_) of the sun, moon and stars, upon the earthly phenomena, they discerned that the celestial objects have a type of lordship _(rububiyyah)_ compared to the earth.\n"
     ]
    }
   ],
   "source": [
    "print(answer_md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
